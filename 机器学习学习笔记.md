# æœºå™¨å­¦ä¹ 

## æµç¨‹

1. æ•°æ®æ¸…æ´—
2. ç‰¹ç§°æŠ½å–
3. æ•°æ®é›†åˆ’åˆ†
4. è®­ç»ƒ
5. é¢„æµ‹



## è¿‡æ‹Ÿåˆ æ¬ æ‹Ÿåˆ

è¿‡æ‹Ÿåˆï¼šè®­ç»ƒé›†å‡†ç¡®ç‡é«˜ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ä½ 

* æ•°æ®é›†å°‘ ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®
* æµ‹è¯•é›†å­˜åœ¨å™ªéŸ³
* æ¨¡å‹å¤æ‚ï¼Œå‚æ•°è¿‡å¤šï¼Œè®­ç»ƒé›†ç›¸å¯¹å°‘ï¼Œä¸è¶³ä»¥æˆç†Ÿåœ°è®­ç»ƒæ¨¡å‹ã€‚ 

æ¬ æ‹Ÿåˆï¼šè®­ç»ƒé›†ã€æµ‹è¯•é›†å‡†ç¡®ç‡éƒ½å¾ˆä½

* ç‰¹å¾è¿‡å°‘ï¼Œä¸è¶³ä»¥è¡¨ç°äº‹ç‰©ç‰¹å¾ 



## éªŒè¯é›† æµ‹è¯•é›†

éªŒè¯é›†ã€æµ‹è¯•é›†ï¼šè®­ç»ƒé›†è‡ªåŠ¨è°ƒæ•´æƒé‡ç³»æ•°ï¼ŒéªŒè¯é›†ä¸å‚ä¸è®­ç»ƒï¼Œ**ç”¨äºæ‰‹åŠ¨è°ƒæ•´æ¨¡å‹è¶…å‚æ•°**ã€‚æµ‹è¯•é›†ç”¨äºè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®ä½¿ç”¨ä¸€æ¬¡ï¼ˆ1.ä¸€èˆ¬æµ‹è¯•é›†æ•°æ®æ¯”è¾ƒå¤§ 2.å¦‚æœæµ‹è¯•é›†ç”¨æ¥è°ƒæ•´è¶…å‚æ•°ï¼Œ**é‚£ä¹ˆä½ çš„æ¨¡å‹äº‹å®ä¸Šåˆå¯èƒ½åœ¨ä½ çš„æµ‹è¯•é›†ä¸Šè¿‡æ‹Ÿåˆäº†ï¼Œå¯¹äºå…¶ä»–çš„æµ‹è¯•é›†å‘¢ï¼Ÿæ³›åŒ–èƒ½åŠ›ä¸å¾—è€ŒçŸ¥**ï¼‰ã€‚ **éªŒè¯é›†ä¸æ˜¯å¿…é¡»çš„ï¼Œå®ƒçš„å­˜åœ¨æ˜¯ä¸ºäº†ç‹¬ç«‹å‡ºæ¨¡å‹è¶…å‚æ•°è°ƒæ•´å’Œæ³›åŒ–èƒ½åŠ›è¯„ä¼°ä¸¤ä¸ªæ­¥éª¤**ã€‚åœ¨è·‘æµ‹è¯•é›†å‰ï¼ŒéªŒè¯é›†ç›¸å½“äºæµ‹è¯•é›†ï¼Œåœ¨è®­ç»ƒè¯¯å·®ã€éªŒè¯è¯¯å·®ä¸­æƒè¡¡ä¸€ç»„ä¸é”™çš„è¶…å‚æ•°ä½œä¸ºæœ€åçš„æ¨¡å‹ï¼Œå»è·‘æµ‹è¯•é›†ã€‚



# Scikit-Learn

`scikit-learn`æ˜¯åŸºäº`Python`è¯­è¨€çš„æœºå™¨å­¦ä¹ å·¥å…·ï¼Œå»ºç«‹åœ¨`NumPy`ï¼Œ`SciPy`å’Œ`matplotlib`ä¸Šã€‚

`Scikit-learn`å†…éƒ¨å®ç°äº†å„ç§å„æ ·æˆç†Ÿçš„ç®—æ³•ï¼Œå®¹æ˜“å®‰è£…å’Œä½¿ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒä¸æ”¯æŒæ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä¸æ”¯æŒå›¾æ¨¡å‹å’Œåºåˆ—é¢„æµ‹ï¼Œä¸æ”¯æŒ`Python`ä¹‹å¤–çš„è¯­è¨€ï¼Œä¸æ”¯æŒ`PyPy`ï¼Œä¹Ÿä¸æ”¯æŒ`GPU`åŠ é€Ÿã€‚

å¦‚æœä¸è€ƒè™‘å¤šå±‚ç¥ç»ç½‘ç»œçš„ç›¸å…³åº”ç”¨ï¼Œ`Scikit-learn`çš„æ€§èƒ½è¡¨ç°æ˜¯éå¸¸ä¸é”™çš„ã€‚ç©¶å…¶åŸå› ï¼Œä¸€æ–¹é¢æ˜¯å› ä¸ºå…¶å†…éƒ¨ç®—æ³•çš„å®ç°ååˆ†é«˜æ•ˆï¼Œå¦ä¸€æ–¹é¢æˆ–è®¸å¯ä»¥å½’åŠŸäº`Cython`ç¼–è¯‘å™¨ï¼šé€šè¿‡`Cython`åœ¨`Scikit-learn`æ¡†æ¶å†…éƒ¨ç”Ÿæˆ`C`è¯­è¨€ä»£ç çš„è¿è¡Œæ–¹å¼ï¼Œ`Scikit-learn`æ¶ˆé™¤äº†å¤§éƒ¨åˆ†çš„æ€§èƒ½ç“¶é¢ˆ

`Scikit-learn`çš„åŸºæœ¬åŠŸèƒ½ä¸»è¦è¢«åˆ†ä¸ºå…­å¤§éƒ¨åˆ†ï¼š`åˆ†ç±»`ï¼Œ`å›å½’`ï¼Œ`èšç±»`ï¼Œ`æ•°æ®é™ç»´`ï¼Œ`æ¨¡å‹é€‰æ‹©`å’Œ`æ•°æ®é¢„å¤„ç†ï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰`ç­‰ã€‚



å®‰è£…ï¼š`pip install scikit-learn`

å®˜æ–¹æ–‡æ¡£ï¼šhttps://scikit-learn.org/stable/user_guide.html 



## fit ,fit_transform

fit_transformæ˜¯fitå’Œtransformçš„ç»“åˆï¼Œ**fitå’Œtransformæ²¡æœ‰ä»»ä½•å…³ç³»ï¼Œä»…ä»…æ˜¯æ•°æ®å¤„ç†çš„ä¸¤ä¸ªä¸åŒç¯èŠ‚ï¼Œfit_transformæ˜¯ä¸€ç§ä¾¿æ·å†™æ³•**ã€‚

fitç›¸å¯¹äºæ•´ä¸ªä»£ç è€Œè¨€ï¼Œä¸ºåç»­APIæœåŠ¡ã€‚**fitä¹‹åï¼Œç„¶åè°ƒç”¨å„ç§APIæ–¹æ³•**ï¼Œtransformåªæ˜¯å…¶ä¸­ä¸€ä¸ªAPIæ–¹æ³•

* fitï¼šæ±‚å¾—è®­ç»ƒé›†Xçš„å›ºæœ‰å±æ€§ï¼Œå¦‚å‡å€¼ï¼Œæ–¹å·®ï¼Œæœ€å¤§å€¼ï¼Œæœ€å°å€¼
* transformï¼šåœ¨fitçš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œæ ‡å‡†åŒ–ï¼Œé™ç»´ï¼Œå½’ä¸€åŒ–ç­‰æ“ä½œ





## ç²¾ç¡®ç‡å’Œå¬å›ç‡

åœ¨äºŒåˆ†ç±»é¢„æµ‹ä¸­

![img](pictures/æ··æ·†çŸ©é˜µ.png)

Tè¡¨ç¤ºé¢„æµ‹æ­£ç¡®ï¼ŒFè¡¨ç¤ºé¢„æµ‹é”™è¯¯ã€‚Pè¡¨ç¤ºé¢„æµ‹é˜³ï¼ŒNè¡¨ç¤ºé¢„æµ‹é˜´ã€‚ **`æ­£ä¾‹æ ·æœ¬=TP+FN`ï¼Œ**



1. å‡†ç¡®ç‡ï¼š$A=\frac{TP+FN}{TP+TN+FP+FN}$ï¼Œé¢„æµ‹ç»“æœä¸å®é™…ä¸€è‡´çš„æ¦‚ç‡ 
2. ç²¾ç¡®ç‡ï¼š$P=\frac{TP}{TP+FP}$ï¼Œ**æ‰€æœ‰æ­£ä¾‹é¢„æµ‹ä¸­**æ ·æœ¬ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥å¾—å‡†ï¼‰
3. å¬å›ç‡ï¼š$R=\frac{TP}{TP+FN}$ï¼Œ**æ‰€æœ‰æ­£ä¾‹æ ·æœ¬ä¸­**é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥çš„å…¨ï¼Œ**å¯¹æ­£æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›**ï¼‰ã€‚
4. ç»¼åˆæŒ‡æ ‡`F1`ï¼š$F1=\frac{2âˆ—Pâˆ—R}{P+R}$ï¼Œç»¼åˆè€ƒè™‘På’ŒRçš„æŒ‡æ ‡



**ç²¾ç¡®ç‡På’Œå¬å›ç‡Rç†è®ºæƒ…å†µä¸‹éƒ½æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œä½†æ˜¯æœºå™¨å­¦ä¹ åŸºäºæ¦‚ç‡é¢„æµ‹ï¼Œè¿™å°±åšä¸åˆ°ä¸¤è€…åŒæ—¶éƒ½å¾ˆé«˜**ã€‚å‡è®¾æ­£ä¾‹æ¦‚ç‡é˜ˆå€¼è®¾çš„è¾ƒå¤§ï¼Œåˆ™ä½œå‡ºçš„æ­£ä¾‹é¢„æµ‹å¾€å¾€éƒ½åŸºäºå¾ˆå¤§æ¦‚ç‡ï¼Œæ‰€ä»¥æŸ¥çš„å‡†ï¼Œä½†ä½œå‡ºæ­£ä¾‹é¢„æµ‹çš„æ¬¡æ•°å°±æ¯”è¾ƒå°‘ï¼ˆæŸ¥çš„ä¸å…¨ï¼‰ã€‚åä¹‹ï¼Œæ¦‚ç‡é˜ˆå€¼è®¾çš„å°ï¼Œåˆ™åšå‡ºçš„é¢„æµ‹åŸºæœ¬éƒ½æ˜¯æ­£ä¾‹é¢„æµ‹ï¼ŒæŸ¥çš„æ¯”è¾ƒå…¨ï¼Œä½†ä¸ä¸€å®šå‡†ï¼Œå‡ºç°å®é™…æ˜¯è´Ÿæ ·æœ¬è€Œä½œå‡ºæ­£ä¾‹é¢„æµ‹çš„æƒ…å†µã€‚



**å‡†ç¡®ç‡Aä¸è¶³ä»¥è¯„ä»·ä¸€ä¸ªæ¨¡å‹çš„å¥½åï¼Œéœ€è¦ç»¼åˆè€ƒè™‘PRæŒ‡æ ‡ã€‚**ä¾‹å¦‚ï¼Œä¸€ä¸ªç”¨äºç™Œç—‡æ£€æµ‹çš„ç©ºç›’å­ï¼Œå¯¹æ‰€æœ‰äººéƒ½è¿”å›â€œå¥åº·â€é¢„æµ‹ï¼Œ100ä¸ªäººä¸­å¦‚æœæœ‰1ä¸ªäººæœ‰ç™Œç—‡ï¼Œå‡†ç¡®ç‡ä¹Ÿæœ‰99%ï¼Œç²¾ç¡®ç‡ä¸º`P=99/100=99%`ï¼Œå¬å›ç‡ä¸º`R=0/1=0%`



**å¤šåˆ†ç±»ä¸­å‡†ç¡®ç‡è®¡ç®—**

```python
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
```

PRæŒ‡æ ‡åº”ç”¨äºå¤šåˆ†ç±»æ ‡ç­¾æ—¶ï¼Œæ˜¯é€ç±»è®¡ç®—çš„ã€‚è®¡ç®—å…¶ä¸­ä¸€ç§åˆ†ç±»çš„æŒ‡æ ‡æ—¶ï¼Œå…¶ä½™åˆ†ç±»çœ‹åšè´Ÿç±»ï¼Œè¯¥ç±»çœ‹åšæ­£ç±»ï¼› 

* microçš„è®¡ç®—æ–¹å¼æ˜¯ç»Ÿè®¡å‡ºæ‰€æœ‰åˆ†ç±»çš„`TP;TN;FN;FP`å†åšè®¡ç®—ï¼Œæ›´åå‘äºæ•´ä¸ªæ•°æ®é›†çš„è¡¨ç°ã€‚
* macroæ˜¯ åˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»çš„`P;R;F`ï¼Œç„¶åæ±‚å¹³å‡ã€‚ ([å‚è€ƒ](https://www.cnblogs.com/techengin/p/8962024.html))





**sklearnä¸­A P R F1æŒ‡æ ‡çš„å•ç‹¬è®¡ç®—**

```python
from sklearn.metrics import precision_score,recall_score,f1_score
from sklearn.neighbors import KNeighborsClassifier

# 1. Kè¿‘é‚»
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(knn,features_temp,targets,cv=5)
print("å‡†ç¡®ç‡ï¼š",scores.mean())

knn.fit(X_train,y_train)

y_predict = knn.predict(X_test)
# ç²¾å‡†ç‡
print("ç²¾å‡†ç‡ï¼š",precision_score(y_test,y_predict))
# å¬å›ç‡
print("å¬å›ç‡ï¼š",recall_score(y_test,y_predict))
# F1-Score
print("F1å¾—åˆ†ï¼š",f1_score(y_test,y_predict))
```





**ç²¾å‡†ç‡å¬å›ç‡æ›²çº¿ç»˜åˆ¶**

å…³é”®æ˜¯å¾—åˆ°ä¸€ç»„PRå€¼ã€‚

 `sklearn.metrics.precision_recall_curve(y_true, probas_pred, pos_label=None)` :

* y_trueï¼šçœŸå®æ ·æœ¬
* probas_predï¼šå¯¹æ­£æ¦‚ç‡çš„é¢„æµ‹æ¦‚ç‡
* pos_labelï¼šæ ·æœ¬å¦‚æœä¸æ˜¯ä»¥0-1ã€-1-1åŒºåˆ†ï¼Œéœ€è¦æ˜¾å¼æŒ‡å®š 

è¿”å›ï¼šä¸‰å…ƒç»„ï¼Œpã€rã€æ¦‚ç‡é˜ˆå€¼ã€‚**åŸç†æ˜¯ç§»åŠ¨ä¸€æ¬¡æ¦‚ç‡é˜ˆå€¼å¾—åˆ°ä¸€å¯¹præŒ‡æ ‡**ã€‚

```python
import numpy as np
from sklearn.metrics import precision_recall_curve
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8]) # æ­£ä¾‹é¢„æµ‹æ¦‚ç‡ 
# precision, recall å¾—åˆ°åå°±å¯ä»¥ç”»å›¾ 
precision, recall, thresholds = precision_recall_curve(
    y_true, y_scores) 
```

```
>>> precision
array([0.66666667, 0.5       , 1.        , 1.        ])
>>> recall
array([1. , 0.5, 0.5, 0. ])
>>> thresholds
array([0.35, 0.4 , 0.8 ])
```

**åœ¨å®é™…è¯„ä¼°æ¨¡å‹æ—¶ï¼Œä¸ºäº†å¾—åˆ°æ­£ä¾‹é¢„æµ‹æ¦‚ç‡åºåˆ—ï¼Œä½¿ç”¨`estimator.predict_proba(features)` ï¼Œè¿”å›æ¦‚ç‡åºåˆ—ã€‚**

```python
print(knn.predict_proba(X_test)) 
'''
ndarray n_samples * n_classes(N,P)  
[[0.4 0.6]
 [0.2 0.8]
 [0.  1. ]
 [1.  0. ]
'''
```



## ROC AUC

ROCæ›²çº¿ç†è§£ï¼šhttps://www.jianshu.com/p/2ca96fce7e81 

```python
from sklearn.metrics import precision_recall_curve,roc_curve,average_precision_score,auc 
def plotting(estimator,y_test):
	'''
	ç»˜åˆ¶ PRæ›²çº¿ ROCæ›²çº¿ 
	'''
    fig,axes = plt.subplots(1,2,figsize=(10,5))
    y_predict_proba = estimator.predict_proba(X_test) # æ¦‚ç‡åºåˆ—
    # PRæ›²çº¿
    precisions,recalls,thretholds = precision_recall_curve(y_test,y_predict_proba[:,1])
    axes[0].plot(precisions,recalls)
    axes[0].set_title("å¹³å‡ç²¾å‡†ç‡ï¼š%.2f"%average_precision_score(y_test,y_predict_proba[:,1]))
    axes[0].set_xlabel("å¬å›ç‡")
    axes[0].set_ylabel("ç²¾å‡†ç‡")
	# ROCæ›²çº¿
    fpr,tpr,thretholds = roc_curve(y_test,y_predict_proba[:,1])
    axes[1].plot(fpr,tpr)
    axes[1].set_title("AUCå€¼ï¼š%.2f"%auc(fpr,tpr)) # AUC é¢ç§¯ 
    axes[1].set_xlabel("FPR")
    axes[1].set_ylabel("TPR")
```



![img](pictures/PRæ›²çº¿ ROCæ›²çº¿.png)



# ç‰¹å¾å·¥ç¨‹

## ç‰¹å¾æŠ½å–

## onehotç¼–ç 

`pandas.get_dummies` ï¼Œå¯¹ç¦»æ•£å‹æ•°æ®ç¼–ç ã€‚

```python
s = pd.Series(list('abca'))
pd.get_dummies(s1)
```

æ¯ä¸€è¡Œä½œä¸ºä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç»´åº¦åˆ†é‡ã€‚

```
   a  b  c
0  1  0  0
1  0  1  0
2  0  0  1
3  1  0  0
```

dfæ•°æ®

```python
df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],
                   'C': [1, 2, 3]})
```

![image-20210218211104788](pictures/image-20210218211104788.png)

```python
pd.get_dummies(df)
# æå–æ¯ä¸€åˆ—çš„å€¼ Cæ˜¯æ•´æ•°å˜é‡ å¹¶æœªå˜åŒ–
```

![image-20210218211209946](pictures/image-20210218211209946.png)

### å­—å…¸ç‰¹å¾æŠ½å–

**å¯¹å­—å…¸çš„éæ•°å€¼å‹é”®å€¼å¯¹è¿›è¡Œone-hotç¼–ç **ã€‚

```python
from sklearn.feature_extraction import DictVectorizer
fruits = [{"fruit":"è‹¹æœ","price":5},
          {"fruit":"æ©˜å­","price":5.9},
          {"fruit":"è è","price":9.9}]  # åˆ—è¡¨ç±»å‹
vect = DictVectorizer()
result = vect.fit_transform(fruits)  # è¿”å›sparse çŸ©é˜µ
```

`result`è¿”å›çš„æ˜¯ä¸€ä¸ª`sparse`çŸ©é˜µï¼Œæ˜¯`scipy`åº“ä¸­çš„æ•°æ®ç±»å‹ã€‚`sparse`çš„ç‰¹ç‚¹æ˜¯å¯ä»¥èŠ‚çœå†…å­˜ï¼Œå› ä¸ºä»–åªä¼šè®°å½•`result.toarray()`è¿™ä¸ªåˆ—è¡¨ä¸­å“ªäº›ä½ç½®å‡ºç°äº†é0çš„å€¼ï¼Œä»¥åŠå…·ä½“çš„å€¼ã€‚

```
(0, 1)	1.0
(0, 3)	5.0
(1, 0)	1.0
(1, 3)	5.9
(2, 2)	1.0
(2, 3)	9.9
```

```python
print(result.toarray()) # è½¬æ¢æˆndarrayç±»å‹æŸ¥çœ‹ ä¸saprseçŸ©é˜µå¯¹æ¯”æŸ¥çœ‹
```

æ¯ä¸€è¡Œçš„æ ¼å¼ï¼š`ont-hotå‘é‡ æ ‡ç­¾`

```
[[0.  1.  0.  5. ] 
 [1.  0.  0.  5.9]
 [0.  0.  1.  9.9]]
```

æ‰“å°ç‰¹å¾å

```python
print(vect.get_feature_names())
# ['fruit=æ©˜å­', 'fruit=è‹¹æœ', 'fruit=è è', 'price']
```



vecå¸¸ç”¨æ–¹æ³•ï¼š

- `fit_transform(X)`ï¼šè½¬åŒ–åˆ—è¡¨`X`ã€‚
- `inverse_transform`ï¼šå°†`numpy`æ•°ç»„æˆ–è€…`sparse`çŸ©é˜µè½¬æ¢æˆåˆ—è¡¨ã€‚
- `get_feature_names()`ï¼šè·å–`fit_transform`åçš„æ•°ç»„ä¸­ï¼Œæ¯ä¸ªä½ç½®ä»£è¡¨çš„æ„ä¹‰ã€‚
- `toarray()`ï¼šå°†`sparse`çŸ©é˜µè½¬æ¢æˆå¤šç»´æ•°ç»„



### æ–‡æœ¬ç‰¹å¾æŠ½å–

`sklearn.feature_extraction.text.CountVectorizer` ï¼Œä»¥è¯ä¸ºå•ä½one-hotç¼–ç ï¼Œä¸”ç»Ÿè®¡ã€‚



```python
from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
# åˆ—è¡¨çš„æ¯ä¸ªå…ƒç´  æ¨¡æ‹Ÿ ä¸€ç¯‡æ–‡ç«  å‡ºç°è‹¥å¹²å•è¯
result = vect.fit_transform(['life is short,i need python',
                             'life is long,i do not need python',
                             'life life life']) # æœ€åä¸€å¥æ˜¯ä¸ºäº†å¯¹æ¯”
print(result.toarray())
print(vect.get_feature_names()) # ç‰¹å¾é¡ºåº 
```

```
[[0 1 1 0 1 0 1 1]
 [1 1 1 1 1 1 1 0]
 [0 0 3 0 0 0 0 0]]
['do', 'is', 'life', 'long', 'need', 'not', 'python', 'short']
```

ç¬¬3å¥çš„lifeå‡ºç°äº†3æ¬¡ã€‚



CountVectorizeråªæ”¯æŒç©ºæ ¼åˆ†è¯ï¼Œå› æ­¤ä¸­æ–‡æ–‡æ¡£éœ€è¦å…ˆä½¿ç”¨è¦ç¬¬ä¸‰æ–¹åº“åˆ†è¯ï¼Œå†ç”¨ç©ºæ ¼è¿ç¼€ã€‚

```python
import jieba
def countvect():
  word1 = "å‘¨å¯†è¯´ï¼Œå—åˆ°å½±å“çš„åº”è¯¥è¯´æ˜¯æŸäº›ç‰¹å®šé¢†åŸŸçš„äº§å“ï¼Œä¸æ˜¯å…¨éƒ¨çš„äº§å“ã€‚"
  word2 = "è£…å‡å›ºç„¶ä¸å¥½ï¼Œå¤„å¤„å¦ç™½ï¼Œä¹Ÿä¸æˆï¼Œè¿™è¦çœ‹æ˜¯ä»€ä¹ˆæ—¶å€™ã€‚å’Œæœ‹å‹è°ˆå¿ƒï¼Œä¸å¿…ç•™å¿ƒï¼Œä½†å’Œæ•Œäººå¯¹é¢ï¼Œå´å¿…é¡»åˆ»åˆ»é˜²å¤‡ï¼Œæˆ‘ä»¬å’Œæœ‹å‹åœ¨ä¸€èµ·ï¼Œå¯ä»¥è„±æ‰è¡£æœï¼Œä½†ä¸Šé˜µè¦ç©¿ç”²ã€‚"
  word3 = "ä¼Ÿå¤§çš„æˆç»©å’Œè¾›å‹¤çš„åŠ³åŠ¨æˆæ­£æ¯”ä¾‹ï¼Œæœ‰ä¸€åˆ†åŠ³åŠ¨å°±æœ‰ä¸€åˆ†æ”¶è·ï¼Œæ—¥ç§¯æœˆç´¯ï¼Œä»å°‘åˆ°å¤šï¼Œå¥‡è¿¹å°±å¯ä»¥åˆ›é€ å‡ºæ¥ã€‚"

  punct = set(u''':!),.:;?]}Â¢'"ã€ã€‚ã€‰ã€‹ã€ã€ã€‘ã€•ã€—ã€ï¸°ï¸±ï¸³ï¹ï½¤ï¹’
ï¹”ï¹•ï¹–ï¹—ï¹šï¹œï¹ï¼ï¼‰ï¼Œï¼ï¼šï¼›ï¼Ÿï½œï½ï¸´ï¸¶ï¸¸ï¸ºï¸¼ï¸¾ï¹€ï¹‚ï¹„ï¹ï½¤ï½ï¿ 
ã€…â€–â€¢Â·Ë‡Ë‰â€•--â€²â€™â€([{Â£Â¥'"â€µã€ˆã€Šã€Œã€ã€ã€”ã€–ï¼ˆï¼»ï½›ï¿¡ï¿¥ã€ï¸µï¸·ï¸¹ï¸»
ï¸½ï¸¿ï¹ï¹ƒï¹™ï¹›ï¹ï¼ˆï½›â€œâ€˜-â€”_â€¦''') # éå¸¸è§„ç¬¦å·
  filterpunt = lambda s: ''.join(filter(lambda x: x not in punct, s)) # è¿‡æ»¤

  con1 = " ".join(jieba.cut(filterpunt(word1))) # è¿ç¼€
  con2 = " ".join(jieba.cut(filterpunt(word2)))
  con3 = " ".join(jieba.cut(filterpunt(word3)))

  vect = CountVectorizer()
  result = vect.fit_transform([con1,con2,con3])
  print(result.toarray())
  print(vect.get_feature_names())
    
countvect() 
```

### tf-idfæ–‡æœ¬æŠ½å–

`tf-idf`ï¼ˆ`tf(term frequency) idf(inverse document frequency)`ï¼‰æ˜¯ä¸€ç§ç”¨äºä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬æŒ–æ˜çš„å¸¸ç”¨åŠ æƒæŠ€æœ¯ã€‚`tf-idf`æ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†çš„é‡è¦ç¨‹åº¦ã€‚**å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”å¢åŠ ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ä¸‹é™ã€‚** 

å‡å¦‚ç°åœ¨æ€»å…±æœ‰1000ä¸ªæ–‡æ¡£ï¼Œâ€œå…±äº«â€è¿™ä¸ªè¯åœ¨500ä¸ªæ–‡æ¡£ä¸­å‡ºç°äº†ï¼Œå¹¶ä¸”`A`æ–‡æ¡£ä¸­æ€»å…±æœ‰800ä¸ªè¯ï¼Œâ€œå…±äº«â€å‡ºç°äº†12æ¬¡ï¼Œé‚£ä¹ˆ`tf-idf`çš„å€¼ä¸ºï¼š`(12/800)*lg(1000/500)`ã€‚ 

```python
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
def tfidf():
    word1 = "å…±äº«å•è½¦ç¡®å®çƒ§é’±ï¼Œä½†è¿™åœ¨ä¸­å›½åˆ›ä¸šåœˆé‡Œå¾ˆå¸¸è§ï¼Œä»æ‰“è½¦åˆ°å¤–å–éƒ½å¹²è¿‡ï¼Œé›·å¸ƒæ–¯ä¹Ÿè¯­é‡å¿ƒé•¿ï¼Œâ€œåˆ›ä¸šè¿˜æ˜¯è¦æœ‰çƒ§ä¸å®Œçš„é’±â€ï¼Œåªä¸è¿‡å…±äº«å•è½¦çƒ§å¾—æ—¢æ²¡å“å‘³ï¼Œä¹Ÿæ²¡æŠ€æœ¯å«é‡ï¼Œæœ€åè¿˜å¼•ç«çƒ§èº«ï¼Œå‡ åäº¿ç¾å…ƒè½å¾—è¿™ä¸ªç»“å±€ï¼Œè°éƒ½æ²¡æƒ³åˆ°ã€‚"
    word2 = "åœ¨äººå·¥æ™ºèƒ½å·²ç»æ·±å…¥ç”Ÿæ´»çš„ä»Šå¤©ï¼Œç¤¾ä¼šä¸Šä¸ä¹â€œäººå·¥æ™ºèƒ½å¨èƒè®ºâ€ï¼Œæ‹…å¿§æœºå™¨äººä¼šâ€œåå™¬â€äººç±»ã€‚åœ¨è¿™ç¯‡æ–‡ç« é‡Œï¼Œæå¼€å¤åšå£«è®¨è®ºäº†äººå·¥æ™ºèƒ½æŠ€æœ¯æœªæ¥å‘å±•æ‰€å¸¦æ¥çš„å‡ ä¸ªæ›´çœŸåˆ‡å’ŒäºŸå¾…è§£å†³çš„é—®é¢˜ï¼šå…¨çƒæ€§çš„å¤±ä¸šé—®é¢˜åŠå¯èƒ½äº§ç”Ÿçš„å…¨çƒæ€§ç»æµå¤±è¡¡å’Œè´«å¯Œå·®è·ã€‚"
    word3 = "ä»2009å¹´åˆ°ç°åœ¨ï¼Œé²¨é±¼æŠ•èµ„è€…è™½è¿˜åœ¨è‚¡æµ·é¨æ¸¸ï¼Œä½†èƒ†å­è¶Šæ¥è¶Šå°ã€‚æŠ˜è…¾äº†Nå¹´ï¼Œå‘ç°è‚¡ç¥¨è¶…å‡ºäº†æˆ‘çš„è®¤çŸ¥èŒƒå›´ï¼Œäº†è§£ä¸€å®¶å…¬å¸ï¼Œéš¾ï¼è‚¡ç¥¨è¡Œæƒ…ï¼Œæ³¢åŠ¨å¤ªå¤§ï¼Œå°å¿ƒè„å—ä¸äº†"
    
	punct = set(u''':!),.:;?]}Â¢'"ã€ã€‚ã€‰ã€‹ã€ã€ã€‘ã€•ã€—ã€ï¸°ï¸±ï¸³ï¹ï½¤ï¹’
ï¹”ï¹•ï¹–ï¹—ï¹šï¹œï¹ï¼ï¼‰ï¼Œï¼ï¼šï¼›ï¼Ÿï½œï½ï¸´ï¸¶ï¸¸ï¸ºï¸¼ï¸¾ï¹€ï¹‚ï¹„ï¹ï½¤ï½ï¿ 
ã€…â€–â€¢Â·Ë‡Ë‰â€•--â€²â€™â€([{Â£Â¥'"â€µã€ˆã€Šã€Œã€ã€ã€”ã€–ï¼ˆï¼»ï½›ï¿¡ï¿¥ã€ï¸µï¸·ï¸¹ï¸»
ï¸½ï¸¿ï¹ï¹ƒï¹™ï¹›ï¹ï¼ˆï½›â€œâ€˜-â€”_â€¦''') # éå¸¸è§„ç¬¦å·
	filterpunt = lambda s: ''.join(filter(lambda x: x not in punct, s)) # è¿‡æ»¤

    
    tf = TfidfVectorizer() 
    con1 = " ".join(jieba.cut(filterpunt(word1)))
    con2 = " ".join(jieba.cut(filterpunt(word2)))
    con3 = " ".join(jieba.cut(filterpunt(word3)))
    result = tf.fit_transform([con1,con2,con3])
    print(result)
    print(tf.get_feature_names())

tfidf()
```

## ç‰¹å¾é¢„å¤„ç†

1. æ•°å€¼å‹æ•°æ®ï¼š
   - å½’ä¸€åŒ–ï¼ˆå°†åŸå§‹æ•°æ®å˜æ¢åˆ°[0,1]ä¹‹é—´ï¼‰
   - æ ‡å‡†åŒ–ï¼ˆæŠŠæ•°æ®è½¬åŒ–åˆ°å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1fèŒƒå›´å†…ï¼‰
   - ç¼ºå¤±å€¼ï¼ˆå°†ç¼ºå¤±å€¼å¤„ç†æˆå‡å€¼ã€ä¸­ä½æ•°ç­‰ï¼‰
2. ç±»åˆ«å‹æ•°æ®ï¼š
   - é™ç»´ï¼ˆå°†å¤šæŒ‡æ ‡è½¬åŒ–ä¸ºå°‘æ•°å‡ ä¸ªç»¼åˆæŒ‡æ ‡ï¼‰
   - PCAï¼ˆé™ç»´çš„ä¸€ç§ï¼‰
3. æ—¶é—´ç±»å‹ï¼š
   - æ—¶é—´çš„åˆ‡åˆ†

### æ ‡å‡†åŒ– 

æ ‡å‡†åŒ–å…¬å¼ï¼š$x=\frac{x-\mu}{\sigma}$ 

<img src="pictures/æ ‡å‡†æ­£æ€åˆ†å¸ƒ.jpg" alt="img" style="zoom: 80%;" />

scikitåº“æœ‰ä¸“é—¨çš„æ ‡å‡†åŒ–å™¨

```python
from sklearn.preprocessing import StandardScaler 
def standard():
  data = [
    [180, 75, 25],
    [175, 80, 19],
    [159, 50, 40],
    [160, 60, 32]
  ]
  scaler = StandardScaler()
  result = scaler.fit_transform(data) # ndarray
  print(result) 
  print("="*10) 
  print(scaler.var_)
```

### ç¼ºå¤±å€¼å¤„ç†

ç¼ºå¤±å€¼ä¸€èˆ¬æœ‰ä¸¤ç§å¤„ç†æ–¹å¼ã€‚ç¬¬ä¸€ç§æ˜¯ç›´æ¥è¿›è¡Œåˆ é™¤ï¼Œç¬¬äºŒç§æ˜¯è¿›è¡Œæ›¿æ¢ã€‚é™¤éç¼ºå¤±å€¼å æ€»æ•°æ®é›†çš„æ¯”ä¾‹éå¸¸å°‘ï¼Œæ‰æ¨èä½¿ç”¨åˆ é™¤çš„æ–¹å¼ï¼Œå¦åˆ™å»ºè®®ä½¿ç”¨â€œå¹³å‡å€¼â€ã€â€œä¸­ä½æ•°â€çš„æ–¹å¼è¿›è¡Œæ›¿æ¢ã€‚åœ¨`scikit-learn`ä¸­ï¼Œæœ‰ä¸“é—¨çš„ç¼ºå¤±å€¼å¤„ç†æ–¹å¼ï¼Œå«åš`sklearn.preprocessing.Imputer`ã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from sklearn.impute import SimpleImputer
def missing():
  im = SimpleImputer()
  data = im.fit_transform([
    [1,2],
    [np.NAN,4],
    [9,1]
  ])
  print(data)
```

### æ•°æ®é™ç»´

ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ç‰¹å¾å¯èƒ½ä¼šæœ‰ä»¥ä¸‹é—®é¢˜ï¼š

1. å†—ä½™ï¼šéƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³åº¦é«˜ï¼Œå®¹æ˜“æ¶ˆè€—è®¡ç®—æ€§èƒ½ã€‚
2. å™ªå£°ï¼šéƒ¨åˆ†ç‰¹å¾å¯¹é¢„æµ‹ç»“æœæœ‰è´Ÿå½±å“ã€‚

#### ç‰¹å¾é€‰æ‹©

ç‰¹å¾é€‰æ‹©ï¼šé€‰æ‹©åˆé€‚çš„ç‰¹å¾ä½œä¸ºè®­ç»ƒç‰¹å¾

1. Filterï¼ˆè¿‡æ»¤å¼ï¼‰ï¼š`VarianceThreshold`ï¼Œåˆ é™¤æ‰€æœ‰ä½æ–¹å·®çš„ç‰¹å¾ã€‚æ–¹å·®è¶Šå°ï¼Œè¯´æ˜æ•°æ®è¶Šé›†ä¸­ï¼Œå¯¹äºæ•´ä¸ªç»“æœå½±å“å¹¶ä¸å¤§
2. Embeddedï¼ˆåµŒå…¥å¼ï¼‰ï¼šæ­£åˆ™åŒ–ã€å†³ç­–æ ‘



è¿‡æ»¤é€‰æ‹©ï¼šè¿‡æ»¤å¼çš„ç‰¹å¾é€‰æ‹©æ˜¯

```python
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler

data = [
  [0, 2, 0, 3],
  [0, 1, 4, 3],
  [0, 1, 1, 3]
]

var = VarianceThreshold(threshold=2)  # æ–¹ç¨‹<=2 çš„åˆ é™¤
result = var.fit_transform(data)
print(result)
```

åªæœ‰ç¬¬3ä¸ªç‰¹å¾è¢«ä¿ç•™

```
[[0]
 [4]
 [1]]
```



#### PCAä¸»æˆåˆ†åˆ†æ

ä¸»æˆåˆ†åˆ†æï¼ˆ`Principal Component Analysisï¼ŒPCA`ï¼‰ï¼Œæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œ**é€šè¿‡æ­£äº¤å˜æ¢å°†ä¸€ç»„å¯èƒ½å­˜åœ¨ç›¸å…³æ€§çš„å˜é‡è½¬æ¢ä¸ºä¸€ç»„çº¿æ€§ä¸ç›¸å…³çš„å˜é‡**ï¼Œè½¬æ¢åçš„è¿™ç»„å˜é‡å«ä¸»æˆåˆ†ã€‚

å¾ˆå¤šæƒ…å½¢ï¼Œå˜é‡ä¹‹é—´æ˜¯æœ‰ä¸€å®šçš„ç›¸å…³å…³ç³»çš„ï¼Œ**ä»–ä»¬è¡¨è¾¾çš„ä¿¡æ¯æœ‰ä¸€å®šçš„é‡å **ã€‚ä¸»æˆåˆ†åˆ†æåšçš„äº‹æƒ…ï¼Œå°±æ˜¯åˆ å»å¤šä½™çš„ï¼ˆå…³ç³»ç´§å¯†ï¼‰çš„å˜é‡ï¼Œå»ºç«‹å°½å¯èƒ½ å°‘çš„æ–°å˜é‡ï¼Œ**ä½¿å¾—è¿™äº›æ–°å˜é‡æ˜¯ä¸¤ä¸¤ä¸ç›¸å…³çš„ï¼Œä¸”å°½å¯èƒ½ä¿æŒåŸæœ‰çš„ä¿¡æ¯**ã€‚



PCAçš„æ€æƒ³ï¼šæœç€æ–¹å·®å¤§çš„æ–¹å‘å˜åŒ–ï¼Œè“è‰²æ˜¯åŸæœ‰åæ ‡(2,2)ï¼ˆå‡è®¾è¿™æ˜¯ä¸€ä¸ªäºŒç»´ç‰¹å¾åˆ—ï¼‰ï¼Œé»„è‰²æ˜¯å˜åŒ–ååæ ‡ï¼Œæ–¹å·®å˜å¤§ã€‚

<img src="pictures/æ­£äº¤å˜æ¢.png" alt="img" style="zoom: 80%;" />



```python
from sklearn.decomposition import PCA

def pca_decomposition():
  pca = PCA(n_components=0.9)
  result = pca.fit_transform(
    [
      [4,2,76,9],
      [1,192,1,56],
      [34,5,20,90]
    ]
  )
  print(result)
```



# åˆ†ç±»

## `scikit-learn`æ•°æ®é›†

`sklearn`å†…éƒ¨æä¾›äº†ä¸€äº›æ•°æ®é›†ï¼Œç”¨äºå­¦ä¹ ä½¿ç”¨ã€‚

### è·å–æ•°æ®é›†

è·å–æ•°æ®é›†çš„æ–¹å¼æœ‰ä¸¤ç§ï¼Œæœ‰`load_*`å’Œ`fetch_*`ä»¥ä¸‹ï¼š

1. `sklearn.datasets.load_*()`ï¼šè·å–**å°è§„æ¨¡**çš„æ•°æ®é›†ï¼Œæ•°æ®é›†å·²ç»éšç€`scikit-learn`å®‰è£…è€Œä¸‹è½½åˆ°æœ¬åœ°äº†ã€‚
2. `sklearn.datasets.fetch_*(data_home=None)`ï¼šç”¨äºä»è¿œç¨‹è·å–**å¤§è§„æ¨¡**çš„æ•°æ®é›†ï¼Œå‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯`data_home`ï¼Œè¡¨ç¤ºæ•°æ®é›†ä¸‹è½½çš„ç›®å½•,é»˜è®¤æ˜¯`~/scikit_learn_data/`ã€‚ã€

ä¸¤ç§æ–¹å¼è¿”å›çš„ç»“æœéƒ½æ˜¯`sklearn.utils.Bunch`ï¼ˆç±»å­—å…¸å¯¹è±¡ï¼Œä¸æ˜¯DataFrameï¼‰ï¼Œè¿™ç§æ•°æ®ç±»å‹æœ‰ä»¥ä¸‹æ–¹æ³•å’Œå±æ€§ï¼š

1. `data`ï¼š**ç‰¹å¾æ•°æ®**æ•°ç»„ï¼Œæ˜¯`[n_samples * n_features]`çš„äºŒç»´`numpy.ndarray`æ•°ç»„ã€‚
2. `target`ï¼š**æ ‡ç­¾**æ•°ç»„ï¼Œæ˜¯`n_samples`çš„ä¸€ç»´`numpy.ndarray`æ•°ç»„ã€‚
3. `DESCR`ï¼šæ•°æ®æè¿°ã€‚
4. `feature_names`ï¼šç‰¹å¾åï¼Œ
5. `target_names`ï¼šæ ‡ç­¾å



#### **æœ¬åœ°æ•°æ®é›†**

æ•°æ®é›†ç½‘å€ï¼šhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_*

åˆ†ç±»æ•°æ®é›†ï¼š

1. `load_iris`ï¼šé¸¢å°¾èŠ±èŠ±ç“£æ•°æ®é›†ã€‚
2. `load_digits`ï¼šæ‰‹å†™æ•°å­—æ•°æ®é›†ã€‚
3. `load_wine`ï¼šçº¢é…’æ•°æ®é›†ã€‚
4. `load_breast_cancer`ï¼šä¹³è…ºç™Œæ•°æ®é›†



å›å½’æ•°æ®é›†

1. `load_boston`ï¼šæ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†ã€‚
2. `load_diabetes`ï¼šç³–å°¿ç—…æ•°æ®é›†ã€‚
3. `load_linnerud`ï¼šä½“èƒ½è®­ç»ƒæ•°æ®é›†ã€‚



#### **è¿œç¨‹æ•°æ®é›†**

1. `fetch_olivetti_faces`ï¼šé¢å­”æ•°æ®é›†ã€‚
2. `fetch_20newsgroups`ï¼š20ä¸ªæ–°é—»ç»„æ•°æ®é›†ã€‚
3. `fetch_lfw_people`ï¼šæˆ·å¤–äººè„¸è¯†åˆ«æ•°æ®é›†ã€‚
4. `fetch_lfw_pairs`ï¼šæˆ·å¤–äººè„¸å¯¹ï¼ˆåŒä¸€ä¸ªäººä¸¤ä¸ªå›¾ç‰‡ï¼‰æ•°æ®é›†ã€‚
5. `fetch_covtype`ï¼šç¾å›½ä¸€å—30*30mçš„æ£®æ—æ–‘å—åœŸåœ°ï¼Œä¸Šé¢è¦†ç›–äº†ä¸åŒç±»å‹çš„æ ‘æœ¨ã€‚
6. `fetch_rcv1`ï¼šè·¯é€ç¤¾æ–‡é›†I (RCV1)ã€‚ç”±è·¯é€ç¤¾æœ‰é™å…¬å¸ä¸ºç ”ç©¶ç›®çš„æä¾›çš„80å¤šä¸‡ç¯‡æ‰‹åŠ¨åˆ†ç±»çš„æ–°é—»ä¸“çº¿æŠ¥é“çš„å­˜æ¡£ã€‚
7. `fetch_california_housing`ï¼šåŠ åˆ©ç¦å°¼äºšæˆ¿å­æ•°æ®é›†



```python
from sklearn import datasets
b_data=datasets.load_boston()  # æœ¬åœ°æ•°æ® 
n_data=datasets.fetch_20newsgroups() # è¿œç¨‹æ•°æ®
```





### æ•°æ®é›†åˆ’åˆ†

åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€æµ‹è¯•é›†ã€‚ä½¿ç”¨`sklearn.model_selection.train_test_split`è¿›è¡Œåˆ†å‰²ã€‚

ä¼ å…¥å‚æ•°ï¼š

1. `x`ï¼šæ•°æ®é›†çš„ç‰¹å¾å€¼ã€‚
2. `y`ï¼šæ•°æ®é›†çš„ç›®æ ‡å€¼ã€‚
3. `test_size`ï¼šæµ‹è¯•çš„æ•°æ®çš„å æ¯”ï¼Œç”¨å°æ•°è¡¨ç¤ºã€‚

è¿”å›å…ƒç»„ï¼Œ`x_train,x_test,y_train,y_test`ï¼Œåˆ†åˆ«è¡¨ç¤ºè®­ç»ƒç‰¹å¾æ•°æ®ã€æµ‹è¯•ç‰¹å¾æ•°æ®ã€è®­ç»ƒæ ‡ç­¾æ•°æ®ã€æµ‹è¯•æ ‡ç­¾æ•°æ®ã€‚

```python
x_train,x_test,y_train,y_test = train_test_split(x=li.data, # ç‰¹å¾æ•°æ®
                                                 y=li.target, # æ ‡ç­¾æ•°æ®
                                                 test_size=0.25) # åˆ†å‰²é›† 
```



## Kè¿‘é‚»ç®—æ³•

### åŸç†

å–æ ·æœ¬å‘¨è¾¹çš„kä¸ªæœ€è¿‘ç‚¹ï¼ˆåŸºäºæ¬§å¼è·ç¦»ï¼‰ï¼Œå¦‚æœå®ƒä»¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ã€‚

<img src="pictures/KNNåŸç†åæ ‡æ¼”ç¤º.png" alt="img" style="zoom:75%;" />

`K`å€¼ä¸èƒ½å–å¤ªå°‘ï¼Œä¹Ÿä¸èƒ½å–å¤ªå¤šã€‚å¦‚æœå–å¤ªå°‘ï¼Œåˆ™å®¹æ˜“å—å¼‚å¸¸ç‚¹çš„å½±å“ï¼Œå¦‚æœå–å¤ªå¤šï¼Œåˆ™å®¹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆçš„ç°è±¡ï¼Œä¹Ÿä¼šå½±å“åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚å› æ­¤è¦ä¸æ–­çš„è°ƒå‚æ•°ï¼Œæ¥çœ‹ä¸‹å‚æ•°å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚



Kè¿‘é‚»ç®—æ³•çš„ä¼˜ç¼ºç‚¹

1. ä¼˜ç‚¹ï¼šç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ— éœ€ä¼°è®¡å‚æ•°ï¼Œè®­ç»ƒæ—¶é—´ä¸º0ã€‚
2. ç¼ºç‚¹ï¼šæ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§ã€‚`K`å€¼è¦ä¸æ–­çš„è°ƒæ•´æ¥è¾¾åˆ°æœ€ä¼˜çš„æ•ˆæœã€‚

åŸºäºä»¥ä¸Šä¼˜ç¼ºç‚¹ï¼Œåœ¨å°æ•°æ®åœºæ™¯ï¼Œå‡ åƒ~å‡ ä¸‡çš„æ•°æ®é‡çš„æ—¶å€™ï¼Œå¯ä»¥ä½¿ç”¨`K`è¿‘é‚»



### å®ç°

`sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')`ï¼š

1. `n_neighbors`ï¼š`int`ï¼Œå¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œ`k_neighbors`æŸ¥è¯¢é»˜è®¤ä½¿ç”¨çš„é‚»å±…æ•°ã€‚
2. `algorithm`ï¼šå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ã€‚
   - `ball_tree`ï¼šå°†ä¼šä½¿ç”¨`BallTree`ã€‚
   - `kd_tree`ï¼šå°†ä½¿ç”¨`KDTree`ã€‚
   - `auto`ï¼šå°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³• (ä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡)



```python
# é¢„æµ‹é…’åˆ†ç±» 
wine_data=datasets.load_wine()
x,y=wine_data.data,wine_data.target

# æ ‡å‡†åŒ– ç‰¹å¾
scaler=StandardScaler()
x=scaler.fit_transform(x)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25) # 25%ç”¨äºæµ‹è¯•

# è®­ç»ƒåˆ†ç±»
knn=KNeighborsClassifier()
knn.fit(x_train,y_train) 
y_predict=knn.predict(x_test)
#print(y_predict)
print('score:%.2f' % knn.score(x_test,y_test)) # æ³¨æ„å‚æ•°æ˜¯ x_test 
```



## æœ´ç´ è´å¶æ–¯

åŸºäºè´å¶æ–¯å…¬å¼$P(A|B)=\frac{P(AB)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}$ ï¼Œç”±ç»“æœæ¨åŸå› ï¼ˆåéªŒæ¦‚ç‡ï¼‰ã€‚



æ–‡æ¡£åˆ†ç±»ä¾‹å­ä¸­è´å¶æ–¯å…¬å¼çš„ä½¿ç”¨

![img](pictures/æœ´ç´ è´å¶æ–¯å…¬å¼_æ–‡æœ¬åˆ†ç±».png)

wè¡¨ç¤ºå…³é”®è¯ï¼Œcè¡¨ç¤ºæ–‡ç« åˆ†ç±»ã€‚$p(c|w)$è¡¨ç¤ºå…³é”®è¯wä¸‹å±äºåˆ†ç±»cçš„æ¦‚ç‡ï¼Œ$p(w|c)$è¡¨ç¤ºæ–‡ç« åˆ†ç±»cä¸‹å‡ºç°å…³é”®è¯wçš„æ¦‚ç‡ã€‚

æ¯”å¦‚ç°åœ¨æœ‰ä»¥ä¸‹è¡¨æ ¼ï¼š

| ç‰¹å¾   | ç§‘æŠ€ç±» | å¨±ä¹ç±» | æ±‡æ€» |
| ------ | ------ | ------ | ---- |
| æ˜æ˜Ÿ   | 9      | 51     | 60   |
| å½±é™¢   | 8      | 56     | 64   |
| äº‘è®¡ç®— | 63     | 0      | 63   |
| æ”¯ä»˜å® | 20     | 15     | 35   |
| æ±‡æ€»   | 100    | 121    | 221  |

å‡å¦‚ç°åœ¨æœ‰ä¸€ç¯‡æ–‡ç« åŒ…å«â€œå½±é™¢â€ã€â€œæ”¯ä»˜å®â€ã€â€œäº‘è®¡ç®—â€ï¼Œè¦è®¡ç®—å±äºç§‘æŠ€ç±»å’Œå¨±ä¹ç±»çš„æ¦‚ç‡ã€‚è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

``` 
P(ç§‘æŠ€|å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—)=P(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—|ç§‘æŠ€)*P(ç§‘æŠ€)/P(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—) 
= (8/100)*(20/100)*(63/100)*(100/221)/P

P(å¨±ä¹|å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—)=P(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—|å¨±ä¹)*P(å¨±ä¹)/P(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—)
= (8/121)*(15/121)*(0/121)*(121/221)/p

åˆ†æ¯éƒ¨åˆ†ç›¸åŒï¼Œæ¯”è¾ƒåˆ†å­å³å¯ï¼Œç§‘æŠ€ çš„æ¦‚ç‡è¦é«˜äº å¨±ä¹ã€‚è¯¥æ–‡ç« å±äº ç§‘æŠ€ çš„å¯èƒ½æ€§è¦é«˜ã€‚

å¨±ä¹ç±»ä¸‹ï¼Œå‡ºç°æ”¯ä»˜å®çš„æ¬¡æ•°ä¸º0ï¼Œæ˜¯å› ä¸ºæ ·æœ¬åå°‘
```

åœ¨å®é™…è®¡ç®—æ—¶ï¼Œä¸ºäº†é˜²æ­¢å¤§é‡0æ¦‚ç‡å‡ºç°ï¼Œä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°ã€‚$ğ‘ƒ(Wâ”‚C)=(ğ‘ğ‘–+ğ›¼)/(ğ‘+ğ›¼ğ‘š)$

1. `ğ‘ğ‘–`ä¸ºè¯¥`W`è¯åœ¨`C`ç±»åˆ«æ‰€æœ‰æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚
2. `N`ä¸ºæ‰€å±ç±»åˆ«`C`ä¸‹çš„æ–‡æ¡£æ‰€æœ‰è¯å‡ºç°çš„æ¬¡æ•°å’Œã€‚
3. `ğ›¼`å°±æ˜¯ä¸ºæŒ‡å®šçš„ç³»æ•°ï¼Œä¸€èˆ¬ä¸º1ã€‚
4. `m`ä¸ºè®­ç»ƒæ–‡æ¡£ä¸­ç»Ÿè®¡å‡ºçš„ç‰¹å¾è¯ä¸ªæ•°ã€‚



```python
# æ–‡ç« åˆ†ç±» è´å¶æ–¯
from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB # å¤šé¡¹å¼è´å¶æ–¯åˆ†ç±»
from sklearn.feature_extraction.text import CountVectorizer # è¯å‘é‡one-hot
from sklearn.model_selection import train_test_split

newsgroups = fetch_20newsgroups(data_home="./data") # dataéƒ¨åˆ†æ¯ä¸€è¡Œæ˜¯ä¸€ç¯‡æ–‡ç«  å­—ç¬¦ä¸²åºåˆ—
X_train,X_test,y_train,y_test = train_test_split(newsgroups.data,newsgroups.target,test_size=0.25)

# å°†æ¯ç¯‡æ–‡ç« è½¬ä¸ºå‘é‡ [word_is_shown ]*vocab_size  
tf = CountVectorizer()
X_train = tf.fit_transform(X_train)
X_test = tf.transform(X_test) # åœ¨ä¸Šä¸€æ­¥å·²ç»fitè¿‡äº†

mnb = MultinomialNB()
mnb.fit(X_train,y_train)
mnb.score(X_test,y_test)
```



**é’ˆå¯¹ä¸åŒçš„ç‰¹å¾æ•°æ®ï¼Œæˆ‘ä»¬æœ‰ä¸åŒçš„æœ´ç´ è´å¶æ–¯æ¨¡å‹æ¥è¿›è¡Œåˆ†ç±»**ã€‚åŸç†éƒ½æ˜¯æœ´ç´ è´å¶æ–¯å…¬å¼ï¼Œåªä¸è¿‡ä¸åŒçš„æ¨¡å‹ï¼Œåœ¨è®¡ç®—æ¦‚ç‡çš„æ—¶å€™é‡‡ç”¨çš„æ–¹å¼ä¸ä¸€æ ·ã€‚

1. å¦‚æœç‰¹å¾æ˜¯ç¦»æ•£å‹æ•°æ®ï¼Œæ¯”å¦‚æ–‡æœ¬è¿™äº›ï¼Œé‚£ä¹ˆæ¨èä½¿ç”¨å¤šé¡¹å¼æ¨¡å‹æ¥å®ç°`sklearn.navie_bayes.MultinomialNB`

2. å¦‚æœç‰¹å¾æ˜¯è¿ç»­å‹æ•°æ®ï¼Œæ¯”å¦‚å…·ä½“çš„æ•°å­—ï¼Œé‚£ä¹ˆæ¨èä½¿ç”¨é«˜æ–¯æ¨¡å‹æ¥å®ç°`sklearn.navie_bayes.GaussianNB` 

   ![img](pictures/é«˜æ–¯åˆ†å¸ƒ.jpg)

3. å¦‚æœç‰¹å¾æ˜¯è¿ç»­å‹æ•°æ®å¹¶ä¸”å€¼åªæœ‰`0`å’Œ`1`ä¸¤ç§æƒ…å†µï¼Œé‚£ä¹ˆæ¨èä½¿ç”¨ä¼¯åŠªåˆ©æ¨¡å‹`sklearn.navie_bayes.BernoulliNB`



æœ´ç´ è´å¶æ–¯å‘æºäºå¤å…¸æ•°å­¦ï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚ç”±äºä½¿ç”¨äº†æ ·æœ¬ç‰¹æ€§æ¡ä»¶ç‹¬ç«‹æ€§ï¼Œå¦‚æœæ ·æœ¬ç‰¹æ€§æœ‰å…³è”çš„æ—¶å€™ï¼Œä¼šå½±å“ç®—æ³•çš„æ•ˆæœ

## å†³ç­–æ ‘

### åŸç†

å†³ç­–æ ‘åˆ†æ”¯ç®—æ³•ï¼š

1. ID3ï¼š`ID3`ç®—æ³•é‡‡ç”¨**ä¿¡æ¯å¢ç›Š**æ¥åˆ›å»ºå†³ç­–æ ‘ã€‚ä¿¡æ¯å¢ç›Šè™½ç„¶æ•ˆæœä¸é”™ï¼Œä½†æ˜¯ä»–æ˜¯åå‘é€‰æ‹©åˆ†æ”¯å¤šçš„å±æ€§ï¼Œä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆï¼Œå› æ­¤ä¸€èˆ¬ä¸ä¼šé‡‡ç”¨æ­¤ç®—æ³•ã€‚ 
2. C4.5ï¼šé‡‡ç”¨**ä¿¡æ¯å¢ç›Šæ¯”**æ¥åˆ›å»ºï¼Œå¯¹åˆ†æ”¯è¿‡å¤šçš„æƒ…å†µè¿›è¡Œæƒ©ç½šï¼Œç®—æ³•æ•ˆç‡åæ…¢
3. CARTï¼šé‡‡ç”¨äº†**åŸºå°¼(Gini)ç³»æ•°**æ¥è¿›è¡Œåˆ†ç±» 

æ¯æ¬¡åˆ†æ”¯é€‰æ‹©å½±å“æœ€å¤§çš„ç‰¹å¾ã€‚



é¢„å‰ªæï¼š

é¢„å‰ªæå°±æ˜¯**åœ¨å†³ç­–æ ‘çš„å»ºç«‹è¿‡ç¨‹ä¸­ä¸æ–­çš„è°ƒèŠ‚**ï¼Œå¯ä»¥è°ƒèŠ‚çš„æ¡ä»¶æœ‰ï¼š

1. æ ‘çš„æ·±åº¦ï¼šå¦‚æœåœ¨å†³ç­–æ ‘å»ºç«‹è¿‡ç¨‹ä¸­ï¼Œå‘ç°æ·±åº¦è¶…è¿‡æŒ‡å®šçš„å€¼ï¼Œé‚£ä¹ˆå°±ä¸å†åˆ†äº†ã€‚
2. å¶å­èŠ‚ç‚¹ä¸ªæ•°ï¼šåŒä¸Šã€‚
3. å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°ï¼šå¦‚æœæŸä¸ªå¶å­èŠ‚ç‚¹çš„ä¸ªæ•°å·²ç»ä½äºæŒ‡å®šçš„å€¼ï¼Œé‚£ä¹ˆå°±ä¸ä¼šå†åˆ†äº†ã€‚
4. ä¿¡æ¯å¢ç›Šé‡/Giniç³»æ•°ï¼šè®¡ç®—ä¿¡æ¯å¢ç›Šé‡æˆ–è€…Giniç³»æ•°ï¼Œå¦‚æœå°äºæŒ‡å®šçš„å€¼ï¼Œé‚£ä¹ˆä¹Ÿä¸ä¼šå†åˆ†äº†ã€‚

ä»¥ä¸Šçš„å‚æ•°ï¼Œéƒ½éœ€è¦åœ¨å»ºæ¨¡çš„è¿‡ç¨‹ä¸­ï¼Œä¸æ–­çš„è°ƒèŠ‚ï¼Œæ¥è¾¾åˆ°æœ€ä¼˜ã€‚

é¢„å‰ªæå¯ä»¥æœ‰æ•ˆçš„é™ä½è¿‡æ‹Ÿåˆç°è±¡ï¼Œæ˜¾è‘—çš„å‡å°‘äº†è®­ç»ƒæ—¶é—´å¼€é”€ã€‚ç”±äºé¢„å‰ªææ˜¯é€šè¿‡é™åˆ¶ä¸€äº›å»ºæ ‘çš„æ¡ä»¶æ¥å®ç°çš„ï¼Œè¿™ç§æ–¹å¼å®¹æ˜“å¯¼è‡´æ¬ æ‹Ÿåˆ



åå‰ªæï¼š

åå‰ªæå°±æ˜¯åœ¨å†³ç­–æ ‘å»ºç«‹å®Œæˆåå†è¿›è¡Œçš„ï¼Œæ¯”è¾ƒå‰ªæå‰åçš„æŸå¤±ï¼Œé€‰æ‹©æŸå¤±å°çš„åšæ³•ã€‚

åå‰ªæé€šå¸¸æ¯”é¢„å‰ªæä¿ç•™æ›´å¤šçš„åˆ†æ”¯ï¼Œæ¬ æ‹Ÿåˆé£é™©æ¯”é¢„å‰ªæè¦å°ï¼Œä½†æ˜¯å› ä¸ºåå‰ªææ˜¯åœ¨æ ‘å»ºç«‹å®Œæˆåå†è‡ªåº•å‘ä¸Šå¯¹æ‰€æœ‰éå¶å­èŠ‚ç‚¹è¿›è¡Œé€ä¸€è€ƒå¯Ÿï¼Œå› æ­¤è®­ç»ƒæ—¶é—´å¼€é”€æ¯”é¢„å‰ªæè¦å¤§å¾—å¤šã€‚

### å®ç°

åœ¨`sklearn`ä¸­ï¼Œå¯ä»¥é€šè¿‡`sklearn.tree.DecisionTreeClassifier`æ¥å®ç°å†³ç­–æ ‘ï¼Œè¿™ä¸ªç±»æœ‰ä»¥ä¸‹å‚æ•°ï¼š

1. **criterion**ï¼š`gini`æˆ–è€…`entropy`çš„æ–¹å¼ã€‚
2. `splitter`ï¼š`best`æˆ–`random`ã€‚`best`æ˜¯åœ¨æ‰€æœ‰ç‰¹ç§ä¸­æ‰¾æœ€å¥½çš„åˆ‡åˆ†ç‚¹ï¼Œ`random`æ˜¯éšæœºçš„æ‰¾ä¸€äº›ç‰¹å¾æ¥è¿›è¡Œåˆ‡åˆ†ï¼ˆæ•°æ®é‡å¤§çš„æ—¶å€™ç”¨`random`ï¼‰
3. **max_depth**ï¼šæ ‘çš„æœ€å¤§æ·±åº¦ã€‚å½“ç‰¹å¾æˆ–è€…æ•°æ®é‡æ¯”è¾ƒå°çš„æ—¶å€™å¯ä»¥ä¸ç”¨ç®¡è¿™ä¸ªå€¼ã€‚ç‰¹å¾æ¯”è¾ƒå¤šçš„æ—¶å€™å¯ä»¥å°è¯•é™åˆ¶ä¸€ä¸‹ã€‚
4. **min_samples_split**ï¼šå†³ç­–æ ‘ä¸­æŸä¸ªå¶å­èŠ‚ç‚¹çš„æ ·æœ¬æœ€å°ä¸ªæ•°ã€‚å¦‚æœæ•°æ®é‡ä¸å¤§ï¼Œä¸éœ€è¦ç®¡è¿™ä¸ªå€¼ï¼Œå¦‚æœæ ·æœ¬é‡æ¯”è¾ƒå¤§ï¼Œåˆ™æ¨èå¢å¤§è¿™ä¸ªå€¼ã€‚
5. `min_weight_fraction_leaf`ï¼šå¶å­èŠ‚ç‚¹æ‰€æœ‰æ ·æœ¬æƒé‡å’Œçš„æœ€å°å€¼ã€‚å¦‚æœå°äºè¿™ä¸ªå€¼ï¼Œåˆ™ä¼šå’Œå…„å¼ŸèŠ‚ç‚¹ä¸€èµ·è¢«å‰ªæï¼Œé»˜è®¤æ˜¯0ï¼Œä¹Ÿå°±æ˜¯ä¸è€ƒè™‘æƒé‡çš„é—®é¢˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¾ƒå¤šæ ·æœ¬æœ‰ç¼ºå¤±å€¼ï¼Œæˆ–è€…åˆ†ç±»æ ‘æ ·æœ¬çš„åˆ†å¸ƒç±»åˆ«åå·®å¾ˆå¤§ï¼Œå°±ä¼šå¼•å…¥æ ·æœ¬æƒé‡ï¼Œè¿™æ—¶æˆ‘ä»¬å°±è¦æ³¨æ„è¿™ä¸ªå€¼äº†ã€‚
6. **max_leaf_nodes**ï¼šæœ€å¤§çš„å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°ã€‚é»˜è®¤æ˜¯`None`ï¼Œå³ä¸é™åˆ¶å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°ã€‚å¦‚æœè®¾ç½®äº†è¿™ä¸ªå€¼ï¼Œé‚£ä¹ˆåœ¨å†³ç­–æ ‘å»ºç«‹çš„è¿‡ç¨‹ä¸­ä¼˜åŒ–å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°ã€‚å¦‚æœç‰¹å¾ä¸å¤šï¼Œå¯ä»¥ä¸è€ƒè™‘è¿™ä¸ªå€¼ï¼Œä½†æ˜¯å¦‚æœç‰¹å¾åˆ†å¤šçš„è¯ï¼Œå¯ä»¥åŠ ä»¥é™åˆ¶ã€‚
7. `class_weight`ï¼šæŒ‡å®šæ ·æœ¬å„ç‰¹å¾çš„æƒé‡ï¼Œä¸»è¦æ˜¯ä¸ºäº†æ–¹å¼æŸäº›ç‰¹å¾çš„æ ·æœ¬è¿‡å¤šå¯¼è‡´åå‘è¿™äº›ç‰¹å¾ã€‚é»˜è®¤æ˜¯`balance`ï¼Œä¹Ÿå°±æ˜¯ç®—æ³•ä¼šè‡ªåŠ¨çš„è°ƒèŠ‚æƒé‡ã€‚
8. **min_impurity_decrease**ï¼šæœ€å°çš„ä¸çº¯åº¦ï¼ˆåŸºå°¼ç³»æ•°ã€ä¿¡æ¯å¢ç›Šç­‰ï¼‰ã€‚å¦‚æœå°äºè¿™ä¸ªæ•°ï¼Œé‚£ä¹ˆå°±ä¸ä¼šå†å¾€ä¸‹ç”Ÿæˆå¶å­èŠ‚ç‚¹äº†ã€‚



```python
# æ³°å¦å°¼å…‹ è·æ•‘é¢„æµ‹
from sklearn.tree import DecisionTreeClassifier  # å†³ç­–æ ‘
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction import DictVectorizer # å­—å…¸å‘é‡
import numpy as np

titanic = pd.read_csv("data/titanic.txt")
features = titanic[['pclass','age','sex']]
features['age'].fillna(features['age'].mean(),inplace=True)  # å¡«å……ç¼ºå¤±
targets = titanic['survived']

X_train,X_test,y_train,y_test = train_test_split(features,targets,test_size=0.25)

vect = DictVectorizer(sparse=False)  # è½¬æ¢å­—å…¸çš„é”®ä¸ºone hotå‘é‡ 
X_train = vect.fit_transform(X_train.to_dict(orient='records'))
X_test = vect.transform(X_test.to_dict(orient="records"))

tree = DecisionTreeClassifier()
tree.fit(X_train,y_train)
tree.score(X_test,y_test)
```



### ç»˜åˆ¶å†³ç­–æ ‘

åœ¨`sklearn`ä¸­ï¼Œå¯ä»¥é€šè¿‡`sklearn.tree.export_graphviz`æ¥å°†æ ‘ç»“æ„å¯¼å‡ºä¸º`.dot`æ–‡ä»¶ï¼Œç„¶åå†åˆ°è¿™ä¸ªç½‘ç«™ï¼š`https://www.graphviz.org/download/`ä¸‹è½½`graphviz`è½¯ä»¶ï¼Œè¿™ä¸ªè½¯ä»¶å¯ä»¥å°†`.dot`æ–‡ä»¶è½¬ä¸º`png`æ ¼å¼ã€‚`graphviz`å®‰è£…å®Œæˆåï¼Œè®°å¾—æŠŠå®‰è£…è·¯å¾„ä¸‹çš„`bin`ç›®å½•ï¼Œæ·»åŠ åˆ°ç¯å¢ƒå˜é‡`PATH`ä¸­ã€‚ç„¶åå†åœ¨cmdç»ˆç«¯ä½¿ç”¨å‘½ä»¤`dot -Tpng ç”Ÿæˆçš„dotæ–‡ä»¶.dot -o tree.png`å°†`dot`æ–‡ä»¶è½¬æ¢ä¸º`png`æ–‡ä»¶ã€‚

å…¶ä¸­`sklearn.tree.export_graphviz`å‡½æ•°æœ‰è®¸å¤šå‚æ•°ï¼Œæˆ‘ä»¬äº†è§£ä¸‹ä»¥ä¸‹å‡ ä¸ªå‚æ•°ï¼š

1. `decision_tree`ï¼šå†³ç­–æ ‘ï¼Œå°±æ˜¯`DecisionTreeClassifier`çš„å¯¹è±¡ã€‚
2. `out_file`ï¼šç”Ÿæˆ`.dot`æ–‡ä»¶çš„è·¯å¾„ã€‚
3. `feature_names`ï¼šç‰¹å¾åç§°ï¼Œæ–¹ä¾¿åœ¨ç”Ÿæˆæ ‘çš„æ—¶å€™èƒ½åœ¨èŠ‚ç‚¹çœ‹åˆ°ç‰¹å¾åç§°ã€‚é€šè¿‡`get_feature_names`æ¥è·å–ï¼Œä¸èƒ½ä¹±å¡«ã€‚
4. `class_names`ï¼šåˆ†ç±»çš„ç±»åã€‚å¦‚æœè®¾ç½®äº†ï¼Œä»–æ˜¯æŒ‰ç…§`target`åˆ†ç±»çš„æ•°å€¼ä»å°åˆ°å¤§è¿›è¡Œæ’åºï¼Œæ‰€ä»¥åœ¨æŒ‡å®šå…·ä½“åç§°çš„æ—¶å€™ï¼Œåº”è¯¥æ ¹æ®æ’åºçš„é¡ºåºæ¥ã€‚

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
export_graphviz(classifier,"tree.dot",feature_names=['age', '1st', '2nd', '3rd', 'female', 'male']
```



## éšæœºæ£®æ—

é›†æˆç®—æ³•åŒ…å«ï¼ˆbagging/boosting/stackingï¼‰åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œéšæœºæ£®æ—æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå†³ç­–æ ‘çš„åˆ†ç±»å™¨ï¼Œå…¶è¾“å‡ºçš„ç±»åˆ«æ˜¯ç”±æ‰€æœ‰æ ‘è¾“å‡ºçš„ç±»åˆ«çš„ä¼—æ•°å†³å®šã€‚ä¾‹å¦‚, å¦‚æœä½ è®­ç»ƒäº†`5`ä¸ªæ ‘, å…¶ä¸­æœ‰`4`ä¸ªæ ‘çš„ç»“æœæ˜¯`True`, 1ä¸ªæ•°çš„ç»“æœæ˜¯`False`, é‚£ä¹ˆæœ€ç»ˆç»“æœä¼šæ˜¯`True` ã€‚

åœ¨å‰é¢çš„å†³ç­–å½“ä¸­æˆ‘ä»¬æåˆ°ï¼Œä¸€ä¸ªæ ‡å‡†çš„å†³ç­–æ ‘ä¼šæ ¹æ®æ¯ç»´ç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„å½±å“ç¨‹åº¦è¿›è¡Œæ’åºï¼Œè¿›è€Œå†³å®šä¸åŒçš„ç‰¹å¾ä»ä¸Šè‡³ä¸‹æ„å»ºåˆ†è£‚èŠ‚ç‚¹çš„é¡ºåºï¼Œå¦‚æ­¤ä»¥æ¥ï¼Œæ‰€æœ‰åœ¨éšæœºæ£®æ—ä¸­çš„å†³ç­–æ ‘éƒ½ä¼šå—è¿™ä¸€ç­–ç•¥å½±å“è€Œæ„å»ºçš„å®Œå…¨ä¸€è‡´ï¼Œä»è€Œä¸§å¤±çš„å¤šæ ·æ€§ã€‚**æ‰€ä»¥åœ¨éšæœºæ£®æ—åˆ†ç±»å™¨çš„æ„å»ºè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€æ£µå†³ç­–æ ‘éƒ½ä¼šæ”¾å¼ƒè¿™ä¸€å›ºå®šçš„æ’åºç®—æ³•ï¼Œè½¬è€Œéšæœºé€‰å–ç‰¹å¾ã€‚** 



å­¦ä¹ ç®—æ³•

1. ç”¨`N`æ¥è¡¨ç¤ºè®­ç»ƒç”¨ä¾‹ï¼ˆæ ·æœ¬ï¼‰çš„ä¸ªæ•°ï¼Œ`M`è¡¨ç¤ºç‰¹å¾æ•°ç›®ã€‚
2. è¾“å…¥ç‰¹å¾æ•°ç›®`m`ï¼Œç”¨äºç¡®å®šå†³ç­–æ ‘èŠ‚ç‚¹åˆ†è£‚ï¼Œå…¶ä¸­`m`åº”è¿œå°äº`M`ã€‚
3. ä»`N`ä¸ªè®­ç»ƒç”¨ä¾‹ï¼ˆæ ·æœ¬ï¼‰ä¸­ä»¥æœ‰æ”¾å›æŠ½æ ·çš„æ–¹å¼ï¼Œå–æ ·`N`æ¬¡ï¼Œå½¢æˆä¸€ä¸ªè®­ç»ƒé›†ï¼ˆå³`bootstrap`å–æ ·ï¼‰ï¼Œå¹¶ç”¨æœªæŠ½åˆ°çš„ç”¨ä¾‹ï¼ˆæ ·æœ¬ï¼‰ä½œé¢„æµ‹ï¼Œè¯„ä¼°å…¶è¯¯å·®ã€‚
4. **å¯¹äºæ¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œéšæœºé€‰æ‹©`m`ä¸ªç‰¹å¾**ï¼Œè®¡ç®—å…¶æœ€ä½³çš„åˆ†è£‚æ–¹å¼ã€‚



`sklearn`å®ç°éšæœºæ£®æ—ï¼š

```
class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=â€™giniâ€™,max_depth=None, bootstrap=True, random_state=None)
```

å‚æ•°ä»‹ç»ï¼š

1. `n_estimators`ï¼š`integer`ï¼Œ`optionalï¼ˆdefault = 10ï¼‰`æ£®æ—é‡Œçš„æ ‘æœ¨æ•°é‡ã€‚
2. `criteria`ï¼š`string`ï¼Œå¯é€‰ï¼ˆdefault =â€œginiâ€ï¼‰åˆ†å‰²ç‰¹å¾çš„æµ‹é‡æ–¹æ³•ã€‚
3. `max_depth`ï¼š`integer`æˆ–`None`ï¼Œå¯é€‰ï¼ˆé»˜è®¤=æ— ï¼‰æ ‘çš„æœ€å¤§æ·±åº¦ ã€‚
4. `bootstrap`ï¼š`boolean`ï¼Œ`optionalï¼ˆdefault = Trueï¼‰`æ˜¯å¦åœ¨æ„å»ºæ ‘æ—¶ä½¿ç”¨æ”¾å›æŠ½æ ·ã€‚



è·å–ç‰¹å¾é‡è¦æ€§ `rnn.feature_importances_`ï¼Œè¿”å›ndarray 

```python
# è·å–éšæœºæ£®æ—è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç‰¹å¾é‡è¦æ€§ å¹¶ä¸ºå…¶ç¼–å†™ç´¢å¼• 
importances = pd.Series(data=knn.feature_importances_,
                        index=features_name).sort_values(ascending=False)
sns.barplot(y=importances.index,x=importances.values,orient='h')
```







# å›å½’

<img src="pictures/å›å½’é€‰æ‹©.png" alt="img" style="zoom:200%;" />

## çº¿æ€§å›å½’

æ­£è§„æ–¹ç¨‹çš„çº¿æ€§å›å½’ç”¨çš„æ˜¯`sklearn.linear_model.LinearRegression`ï¼Œæ¢¯åº¦ä¸‹é™ç”¨çš„æ˜¯`sklearn.linear_model.SGDRegressor`ã€‚

åœ¨é¢„æµ‹å®Œæˆåï¼Œæˆ‘ä»¬æƒ³è¦çŸ¥é“é¢„æµ‹çš„å¥½åçš„æŒ‡æ ‡ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç§æ–¹å¼è¿›è¡Œåˆ¤åˆ«ï¼š

1. å‡æ–¹è¯¯å·®ï¼š`sklearn.metrics.mean_squared_error(y_true, y_pred)`ã€‚ï¼ˆè¯¯å·®å¹³æ–¹å’Œçš„å‡å€¼ï¼‰ã€‚
2. å¹³å‡ç»å¯¹è¯¯å·®ï¼š`sklearn.metrics.mean_absolute_error(y_true, y_pred)`ï¼ˆè¯¯å·®çš„å¹³å‡ï¼‰ã€‚

éƒ½æ˜¯è¶Šå°è¶Šå¥½ã€‚



```python
# ç”¨çº¿æ€§å›å½’æ¥é¢„æµ‹æ³¢å£«é¡¿æˆ¿ä»·ã€‚
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression,SGDRegressor  # çº¿æ€§å›å½’,SGD
from sklearn.metrics import mean_squared_error  # å‡æ–¹è¯¯å·®

# åŠ è½½æ•°æ®
boston = load_boston()
# åˆ†å‰²æ•°æ®
feature_train,feature_test,target_train,target_test = train_test_split(boston.data,boston.target)

# æ ‡å‡†åŒ–æ•°æ®
scaler_feature = StandardScaler()  # æ ‡å‡†åŒ– ç‰¹å¾
feature_train = scaler_feature.fit_transform(feature_train)
feature_test = scaler_feature.transform(feature_test)

scaler_target = StandardScaler() # æ ‡å‡†åŒ–æ ‡ç­¾
target_train = scaler_target.fit_transform(target_train.reshape(-1,1))
target_test = scaler_target.transform(target_test.reshape(-1,1))

# çº¿æ€§å›å½’(æ­£è§„æ–¹ç¨‹)
linear = LinearRegression()  
linear.fit(feature_train,target_train)
predict_target_test = scaler_target.inverse_transform(linear.predict(feature_test)) #æ‹Ÿæ ‡å‡† 
# è®¡ç®—ä¸‹æ­£è§„æ–¹ç¨‹çš„æ•ˆæœ
mean_squared_error(scaler_target.inverse_transform(target_test),predict_target_test)

# çº¿æ€§å›å½’ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
sgd = SGDRegressor()
sgd.fit(feature_train,target_train)
predict_target_test = scaler_target.inverse_transform(sgd.predict(feature_test))
# è®¡ç®—ä¸‹æ¢¯åº¦ä¸‹é™çš„æ•ˆæœ
mean_squared_error(scaler_target.inverse_transform(target_test),predict_target_test)
```

## å²­å›å½’

æ­£åˆ™åŒ–å‡ºç°çš„ç›®æ ‡ï¼Œå°±æ˜¯ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆçš„ç°è±¡ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$J(Î¸)=MSE(Î¸)+Î»\frac12âˆ‘_i^nÎ¸_i^2$ 

`Î»`è¶Šå¤§ï¼Œè¯´æ˜æƒ©ç½šåŠ›åº¦æ˜¯è¶Šå¤§çš„ã€‚ä½†æ˜¯å¹¶ä¸æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œå¤ªå¤§äº†ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å¤„ç†æ‹Ÿåˆå¾—ä¸å¥½ï¼Œå¯¼è‡´æœ€ç»ˆé¢„æµ‹è¯„åˆ†æ›´ä½ã€‚

å²­å›å½’ï¼Œå°±æ˜¯åŠ å…¥äº†æ­£åˆ™æƒ©ç½šé¡¹çš„å›å½’ï¼Œå¯ä»¥ç”¨`sklearn.linear_model.Ridge`æ¥å®ç°ï¼š

```python
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1)  # lambda
ridge.fit(X_train,y_train)
y_predict = ridge.predict(X_test)
print("å¹³å‡è¯¯å·®ï¼š",mean_absolute_error(y_test,y_predict))
print("å‡æ–¹è¯¯å·®ï¼š",mean_squared_error(y_test,y_predict))
```



## é€»è¾‘å›å½’

é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰ï¼Œç®€ç§°LRã€‚å®ƒçš„ç‰¹ç‚¹æ˜¯èƒ½å¤Ÿæ˜¯æˆ‘ä»¬çš„ç‰¹å¾è¾“å…¥é›†åˆè½¬åŒ–ä¸º0å’Œ1è¿™ä¸¤ç±»çš„æ¦‚ç‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå›å½’ä¸ç”¨åœ¨åˆ†ç±»é—®é¢˜ä¸Šï¼Œä½†é€»è¾‘å›å½’å´åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸Šè¡¨ç°å¾ˆå¥½ã€‚é€»è¾‘å›å½’æœ¬è´¨ä¸Šæ˜¯çº¿æ€§å›å½’ï¼Œåªæ˜¯åœ¨ç‰¹å¾åˆ°ç»“æœçš„æ˜ å°„ä¸­åŠ å…¥äº†ä¸€å±‚`Sigmod`å‡½æ•°æ˜ å°„ï¼Œå³å…ˆæŠŠç‰¹å¾çº¿æ€§æ±‚å’Œï¼Œç„¶åä½¿ç”¨`Sigmoid`å‡½æ•°å°†æœ€ä¸ºå‡è®¾å‡½æ•°æ¥æ¦‚ç‡æ±‚è§£ï¼Œå†è¿›è¡Œåˆ†ç±»ã€‚



```python
# ç™Œç—‡æ‚£è€…äºŒåˆ†ç±» æ¡ˆä¾‹

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report  # æŸ¥çœ‹ç²¾ç¡®ç‡ å¬å›ç‡

# åœ°å€è·¯å¾„
names = ["Sample code number ","Clump Thickness","Uniformity of Cell Size","Uniformity of Cell Shape","Marginal Adhesion","Single Epithelial Cell Size","Bare Nuclei","Bland Chromatin","Mitoses","Class"]
breast = pd.read_csv("./data/breast-cancer-wisconsin.data",names=names)
breast = breast.replace(to_replace="?",value=np.nan)
breast = breast.dropna()

# åˆ‡åˆ†æ•°æ®
x_train,x_test,y_train,y_test = train_test_split(breast[names[1:-1]],breast[names[-1]],test_size=0.25)

# æ ‡å‡†åŒ–æ•°æ®
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# é€»è¾‘å›å½’
lg = LogisticRegression(C=1.0)
lg.fit(x_train,y_train)
y_predict = lg.predict(x_test)
print("å‡†ç¡®ç‡ï¼š",lg.score(x_test,y_test))
print("å¬å›ç‡ï¼š",classification_report(y_test,y_predict,labels=[2,4],target_names=['è‰¯æ€§','æ¶æ€§']))
```



# äº¤å‰éªŒè¯ä¸ç½‘æ ¼æœç´¢

## äº¤å‰éªŒè¯

`sklearn.cross_validation.cross_val_score` 

```python
def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')
  """
  :param estimator:æ¨¡å‹ä¼°è®¡å™¨
  :param X:ç‰¹å¾æ•°æ®
  :param y:æ ‡ç­¾æ•°æ®
  :param cv:intï¼Œä½¿ç”¨é»˜è®¤çš„3æŠ˜äº¤å‰éªŒè¯ï¼Œæ•´æ•°æŒ‡å®šä¸€ä¸ªï¼ˆåˆ†å±‚ï¼‰KFoldä¸­çš„æŠ˜å æ•°
  :return :é¢„ä¼°ç³»æ•°
  """
```

ç”¨äº¤å‰éªŒè¯ä¼°è®¡kè¿‘é‚»æ¨¡å‹çš„å‡†ç¡®ç‡ 

```python
from sklearn.model_selection import cross_val_score 
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(knn,features_temp,targets,cv=5) # 5æŠ˜äº¤å‰éªŒè¯ è¿”å›5ç»„åˆ†æ•°
print("å‡†ç¡®ç‡ï¼š",scores.mean())  # å‡åˆ† 
```





ä¸€èˆ¬æˆ‘ä»¬çš„ä¸€ä¸ªæ¨¡å‹å¥½åï¼Œæ˜¯å¯ä»¥é€šè¿‡è°ƒèŠ‚â€œè¶…å‚æ•°â€æ¥è¿›è¡Œæ”¹è¿›çš„ï¼Œä½†æ˜¯ä¸€ä¸ªâ€œè¶…å‚æ•°â€éƒ½å¯èƒ½æœ‰å¤šç§é€‰æ‹©ï¼Œå…·ä½“å“ªç§é€‰æ‹©æ›´å¥½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç½‘æ ¼æœç´¢æ¥å®ç°ã€‚ç½‘æ ¼æœç´¢ä¼šæ ¹æ®ä½ æä¾›çš„å‚æ•°ï¼Œåˆ†åˆ«è¿›è¡ŒåŒ¹é…ï¼Œä»¥æŸ¥æ‰¾åˆ°æœ€ä½³çš„å‚æ•°ç»„åˆã€‚åº•å±‚çš„ä»£ç ï¼Œå°±ç”¨åˆ°äº†äº¤å‰éªŒè¯ã€‚ç½‘æ ¼æœç´¢å¯ä»¥ç”¨`sklearn.model_selection.GridSearchCV`æ¥å®ç°ã€‚

```python
knn = KNeighborsClassifier()
# é€šè¿‡ç½‘æ ¼æœç´¢,n_neighborsä¸ºå‚æ•°åˆ—è¡¨
param = {"n_neighbors": [3, 5, 7]}
gs = GridSearchCV(knn, param_grid=param, cv=10)
# å»ºç«‹æ¨¡å‹
gs.fit(x_train,y_train)
# print(gs)
# é¢„æµ‹æ•°æ®
print(gs.score(x_test,y_test))
```

